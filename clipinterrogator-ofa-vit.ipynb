{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## About this notebook\n","This notebook ensembles the CLIP Interrogator, OFA model and ViT model."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-10T06:58:37.040756Z","iopub.status.busy":"2023-04-10T06:58:37.039506Z","iopub.status.idle":"2023-04-10T06:58:37.046503Z","shell.execute_reply":"2023-04-10T06:58:37.045356Z","shell.execute_reply.started":"2023-04-10T06:58:37.040714Z"},"trusted":true},"outputs":[],"source":["ratio_ViT_B_16          = 0.74880\n","ratio_CLIP_Interrogator = 0.21120\n","ratio_OFA               = 0.04000"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### <a id=\"top\"></a>\n","# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b> OFA </b></div>\n","\n","[Original notebook](https://www.kaggle.com/code/mayukh18/ofa-transformer-lb-0-42644)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-04-10T06:58:37.049868Z","iopub.status.busy":"2023-04-10T06:58:37.048734Z"},"trusted":true},"outputs":[],"source":["# Using the pre compiled wheel since we don't have internet on submission\n","!pip install -q /kaggle/input/stable-diffusion-data/transformers-4.18.0.dev0-py3-none-any.whl"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import sys\n","import glob\n","from pathlib import Path\n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","\n","from PIL import Image\n","import torch\n","from torch.utils.data import Dataset\n","from torchvision import transforms\n","from transformers import OFATokenizer, OFAModel\n","from transformers.models.ofa.generate import sequence_generator\n","\n","import gc"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["CKPT_DIR = \"/kaggle/input/stable-diffusion-data/OFA-large-caption/\"\n","IMAGE_DIR = \"/kaggle/input/stable-diffusion-image-to-prompts/images\"\n","\n","BATCH_SIZE = 24"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Loading the pretrained OFA model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["mean, std = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n","resolution = 480\n","patch_resize_transform = transforms.Compose([\n","        lambda image: image.convert(\"RGB\"),\n","        transforms.Resize((resolution, resolution), interpolation=Image.BICUBIC),\n","        transforms.ToTensor(), \n","        transforms.Normalize(mean=mean, std=std)\n","    ])\n","\n","tokenizer = OFATokenizer.from_pretrained(CKPT_DIR)\n","model = OFAModel.from_pretrained(CKPT_DIR, use_cache=False).cuda()\n","txt = \" what does the image describe?\"\n","inputs = tokenizer([txt], return_tensors=\"pt\").input_ids"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Model EDA"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_images = glob.glob(\"/kaggle/input/stable-diffusion-image-to-prompts/images/*\")[:7]\n","fig, ax = plt.subplots(7,1, figsize=(4,35))\n","\n","for i,impath in enumerate(sample_images):\n","    image = Image.open(impath)\n","    image_t = patch_resize_transform(image).cuda().unsqueeze(0)\n","    out = model.generate(inputs.cuda(), patch_images=image_t.cuda(), num_beams=5, no_repeat_ngram_size=2)\n","    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n","    ax[i].imshow(image)\n","    ax[i].text(1.1, .5, out_captions[0], horizontalalignment='left', verticalalignment='center', transform=ax[i].transAxes)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Inference"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n","from sentence_transformers import SentenceTransformer, models\n","\n","comp_path = Path('../input/stable-diffusion-image-to-prompts/')\n","st_model = SentenceTransformer('/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class ImageGen(Dataset):\n","    def __init__(self, root, batch_size=32):\n","        self.root = root\n","        self.im_paths = os.listdir(self.root)\n","        self.batch_size = batch_size\n","        self.sz = len(self.im_paths)\n","        self.genlen = self.sz//self.batch_size + int(self.sz%self.batch_size > 0)\n","        \n","    def __getitem__(self, index):\n","        if index >= self.genlen:\n","            raise IndexError(\"Out of bounds\")\n","        \n","        l, r = index*self.batch_size, min(self.sz, (index+1)*self.batch_size)\n","        \n","        f_paths = [os.path.join(self.root, self.im_paths[i]) for i in range(l,r)]\n","        f_ids = [self.im_paths[i][:-4] for i in range(l,r)]\n","        \n","        ims = [Image.open(f_path) for f_path in f_paths]\n","        ims = [patch_resize_transform(im).cuda().unsqueeze(0) for im in ims]\n","        ims = torch.cat(ims)\n","        \n","        return ims, f_ids\n","    \n","    def __len__(self):\n","        return self.genlen"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sub_ids = []\n","sub_embeds = []\n","\n","imgen = ImageGen(IMAGE_DIR, BATCH_SIZE)\n","\n","for b in imgen:\n","    for j in range(len(b[1])):\n","        sub_ids.extend([f\"{b[1][j]}_{i}\" for i in range(384)])\n","    \n","    img_batch = b[0]\n","    out = model.generate(inputs.repeat(len(img_batch), 1).cuda(), patch_images=img_batch, num_beams=5, no_repeat_ngram_size=2)\n","    out_captions = tokenizer.batch_decode(out, skip_special_tokens=True)\n","    out_captions = [cap + \", fine details, masterpiece\" for cap in out_captions]\n","    \n","    embeddings = st_model.encode(out_captions).flatten()\n","    sub_embeds.extend(embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embeddings1 = np.array(sub_embeds)\n","embeddings1.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del model, tokenizer, st_model\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### <a id=\"top\"></a>\n","# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b>CLIP Interrogator</b></div>\n","\n","[original notebook](https://www.kaggle.com/code/leonidkulyk/lb-0-45836-blip-clip-clip-interrogator)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"1\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 1. About CLIP interrogator tool</b></div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">⚪ The CLIP Interrogator is a prompt engineering tool that combines OpenAI's <a href=\"https://openai.com/blog/clip/\"><strong>CLIP</strong></a> and Salesforce's <a href=\"https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/\"><strong>BLIP</strong></a> to optimize text prompts to match a given image.</p>\n","\n","<p style=\"font-family: consolas; font-size: 16px;\">⚪ CLIP Interrogator uses OpenCLIP which supports many different pretrained CLIP models. For the best prompts for Stable Diffusion 2.0 uses <b>ViT-H-14/laion2b_s32b_b79k</b>.</p>\n","\n","\n","<p style=\"font-family: consolas; font-size: 16px;\">⚪ CLIP Interrogator pipeline looks as follows:</p>\n","\n","* <p style=\"font-family: consolas; font-size: 16px;\">An image is passed to the input to BLIP to obtain the main description.</p>\n","\n","* <p style=\"font-family: consolas; font-size: 16px;\">An image is passed to the input to CLIP to receive its embedding.</p>\n","\n","* <p style=\"font-family: consolas; font-size: 16px;\">Embeddings received from the image are compared with embeddings received from labels from the lists and the top 4 with the greatest similarity are selected.</p>\n","<p style=\"font-family: consolas; font-size: 16px;\">There are 4 main lists on which the outgoing prompt for the CLIP part is formed: <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/artists.txt\"><strong>artists.txt</strong></a> (list with artists), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/flavors.txt\"><strong>flavors.txt</strong></a> (main list for image description), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/mediums.txt\"><strong>mediums.txt</strong></a> (image type), <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/data/movements.txt\"><strong>movements.txt</strong></a> (image style) and <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/clip_interrogator.py#L115\"><strong>sites</strong></a> (popular artwork sites). As I wrote earlier, removing the <b>artists.txt</b> and the <b>sites</b> lists can significantly improve the output score.</p>\n","\n","* <p style=\"font-family: consolas; font-size: 16px;\">The resulting texts are concatenated and returned as an image description (or promt on which an image was generated).</p>\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">🔴 CLIP Interrogator pipeline, schematic image [<a href=\"https://medium.com/@silkworm/diversify-photo-database-with-clip-interrogator-5dd1833be9f5\"><strong>source</strong></a>]:</p>\n","\n","![](https://user-images.githubusercontent.com/45982614/220214422-19529ba3-9c13-40cd-a3a6-434785002974.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">⚪ If you want, you can experiment with this tool on the Hugging Face Space -- *<a href=\"https://huggingface.co/spaces/pharma/CLIP-Interrogator\"><strong>Click</strong></a>*. Example of an output:</p>\n","\n","![](https://user-images.githubusercontent.com/45982614/220215304-d7e79716-35a2-4f29-867f-57ca996aab2a.png)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"2\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 2. Install & Import all dependencies </b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["wheels_path = \"/kaggle/input/clip-interrogator-wheels-x\"\n","clip_interrogator_whl_path = f\"{wheels_path}/clip_interrogator-0.4.3-py3-none-any.whl\""]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["!pip install --no-index --find-links $wheels_path $clip_interrogator_whl_path -q"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["!pip list | grep transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"trusted":true},"outputs":[],"source":["import inspect\n","import importlib\n","\n","from blip.models import blip\n","from clip_interrogator import clip_interrogator"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# replace tokenizer path to prevent downloading\n","blip_path = inspect.getfile(blip)\n","\n","fin = open(blip_path, \"rt\")\n","data = fin.read()\n","data = data.replace(\n","    \"BertTokenizer.from_pretrained('bert-base-uncased')\", \n","    \"BertTokenizer.from_pretrained('/kaggle/input/clip-interrogator-models-x/bert-base-uncased')\"\n",")\n","fin.close()\n","\n","fin = open(blip_path, \"wt\")\n","fin.write(data)\n","fin.close()\n","\n","# reload module\n","importlib.reload(blip)"]},{"cell_type":"code","execution_count":null,"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"outputs":[],"source":["# fix clip_interrogator bug\n","clip_interrogator_path = inspect.getfile(clip_interrogator.Interrogator)\n","\n","fin = open(clip_interrogator_path, \"rt\")\n","data = fin.read()\n","data = data.replace(\n","    'open_clip.get_tokenizer(clip_model_name)', \n","    'open_clip.get_tokenizer(config.clip_model_name.split(\"/\", 2)[0])'\n",")\n","fin.close()\n","\n","fin = open(clip_interrogator_path, \"wt\")\n","fin.write(data)\n","fin.close()\n","\n","# reload module\n","importlib.reload(clip_interrogator)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import sys\n","from PIL import Image\n","from pathlib import Path\n","import matplotlib.pyplot as plt \n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import open_clip\n","\n","\n","sys.path.append('../input/sentence-transformers-222/sentence-transformers')\n","from sentence_transformers import SentenceTransformer, models\n","\n","comp_path = Path('/kaggle/input/stable-diffusion-image-to-prompts/')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"3\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 3. Set configs</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CFG:\n","    device = \"cuda\"\n","    seed = 42\n","    embedding_length = 384\n","    sentence_model_path = \"/kaggle/input/sentence-transformers-222/all-MiniLM-L6-v2\"\n","    blip_model_path = \"/kaggle/input/clip-interrogator-models-x/model_large_caption.pth\"\n","    ci_clip_model_name = \"ViT-H-14/laion2b_s32b_b79k\"\n","    clip_model_name = \"ViT-H-14\"\n","    clip_model_path = \"/kaggle/input/clip-interrogator-models-x/CLIP-ViT-H-14-laion2B-s32B-b79K/open_clip_pytorch_model.bin\"\n","    cache_path = \"/kaggle/input/clip-interrogator-models-x\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"4\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 4. Load the Sample Submission</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_submission = pd.read_csv(comp_path / 'sample_submission.csv', index_col='imgId_eId')\n","df_submission.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"5\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 5. Build index from images</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = os.listdir(comp_path / 'images')\n","imgIds = [i.split('.')[0] for i in images]\n","\n","eIds = list(range(CFG.embedding_length))\n","\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, CFG.embedding_length),\n","        np.tile(range(CFG.embedding_length), len(imgIds))\n","    )\n","]\n","\n","assert sorted(imgId_eId) == sorted(df_submission.index)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"6\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 6. Load the embedding model</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["st_model = SentenceTransformer(CFG.sentence_model_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"7\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 7. Prepare CLIP interrogator tool</b></div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"7.1\"></a>\n","## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.1 Define CLIP interrogator config</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model_config = clip_interrogator.Config(clip_model_name=CFG.ci_clip_model_name)\n","model_config.cache_path = CFG.cache_path"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"7.2\"></a>\n","## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.2 Define BLIP model</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["configs_path = os.path.join(os.path.dirname(os.path.dirname(blip_path)), 'configs')\n","med_config = os.path.join(configs_path, 'med_config.json')\n","blip_model = blip.blip_decoder(\n","    pretrained=CFG.blip_model_path,\n","    image_size=model_config.blip_image_eval_size, \n","    vit=model_config.blip_model_type, \n","    med_config=med_config\n",")\n","blip_model.eval()\n","blip_model = blip_model.to(model_config.device)\n","model_config.blip_model = blip_model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"7.3\"></a>\n","## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.3 Define CLIP model</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clip_model = open_clip.create_model(CFG.clip_model_name, precision='fp16' if model_config.device == 'cuda' else 'fp32')\n","open_clip.load_checkpoint(clip_model, CFG.clip_model_path)\n","clip_model.to(model_config.device).eval()\n","model_config.clip_model = clip_model"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["clip_preprocess = open_clip.image_transform(\n","    clip_model.visual.image_size,\n","    is_train = False,\n","    mean = getattr(clip_model.visual, 'image_mean', None),\n","    std = getattr(clip_model.visual, 'image_std', None),\n",")\n","model_config.clip_preprocess = clip_preprocess"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"7.4\"></a>\n","## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 7.4 Create CLIP interrogator object</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["ci = clip_interrogator.Interrogator(model_config)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"8\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 8. Define interrogate function</b></div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"8.1\"></a>\n","## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 8.1 Get labels embeddings</b></div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">⚪ Original CLIP Interrogator uses image_features and text_embeds matrix multiplication to fine the similarity between the corresponding image and text label. But I found that using cosine similarity is much faster and the resulting score is almost identical. So take that into account.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["cos = torch.nn.CosineSimilarity(dim=1)\n","\n","mediums_features_array = torch.stack([torch.from_numpy(t) for t in ci.mediums.embeds]).to(ci.device)\n","movements_features_array = torch.stack([torch.from_numpy(t) for t in ci.movements.embeds]).to(ci.device)\n","flavors_features_array = torch.stack([torch.from_numpy(t) for t in ci.flavors.embeds]).to(ci.device)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"8.2\"></a>\n","## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 8.2 Create main interrogation function</b></div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<p style=\"font-family: consolas; font-size: 16px;\">⚪ It's modified version of the original <a href=\"https://github.com/pharmapsychotic/clip-interrogator/blob/main/clip_interrogator/clip_interrogator.py#L213\"><strong>interrogate_classic</strong></a> method.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def interrogate(image: Image) -> str:\n","    caption = ci.generate_caption(image)\n","    image_features = ci.image_to_features(image)\n","    \n","    medium = [ci.mediums.labels[i] for i in cos(image_features, mediums_features_array).topk(1).indices][0]\n","    movement = [ci.movements.labels[i] for i in cos(image_features, movements_features_array).topk(1).indices][0]\n","    flaves = \", \".join([ci.flavors.labels[i] for i in cos(image_features, flavors_features_array).topk(3).indices])\n","\n","    if caption.startswith(medium):\n","        prompt = f\"{caption}, {movement}, {flaves}\"\n","    else:\n","        prompt = f\"{caption}, {medium}, {movement}, {flaves}\"\n","\n","    return clip_interrogator._truncate_to_fit(prompt, ci.tokenize)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"9\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 9. Extract promt from images</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["prompts = []\n","\n","images_path = \"../input/stable-diffusion-image-to-prompts/images/\"\n","for image_name in images:\n","    img = Image.open(images_path + image_name).convert(\"RGB\")\n","\n","    generated = interrogate(img)\n","    \n","    prompts.append(generated)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"9.1\"></a>\n","## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 9.1 Check the result</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def add_text_limiters(text: str) -> str:\n","    return \" \".join([\n","        word + \"\\n\" if i % 15 == 0 else word \n","        for i, word in enumerate(text.split(\" \"), start=1)\n","    ])\n","\n","def plot_image(image: np.ndarray, original_prompt: str, generated_prompt: str) -> None:\n","    plt.figure(figsize=(10, 10))\n","    plt.imshow(image)\n","    plt.annotate(\n","        \"Original prompt:\\n\" + add_text_limiters(original_prompt) + \"\\n\\nGenerated prompt:\\n\" + add_text_limiters(generated_prompt), \n","        xy=(1.05, 0.5), xycoords='axes fraction', ha='left', va='center', \n","        fontsize=16, rotation=0, color=\"#104a6e\"\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# DO NOT FORGET TO COMMENT OUT THIS CELL DURING SUBMISSION\n","\n","#original_prompts_df = pd.read_csv(\"/kaggle/input/stable-diffusion-image-to-prompts/prompts.csv\")\n","\n","#for image_name, prompt in zip(images, prompts):\n","#    img = Image.open(images_path + image_name).convert(\"RGB\")\n","#    original_prompt = original_prompts_df[\n","#        original_prompts_df.imgId == image_name.split(\".\")[0]\n","#    ].prompt.iloc[0]\n","#    plot_image(img, original_prompt, prompt)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"10\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 10. Create a sample submission with a constant prompt prediction</b></div>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"10.1\"></a>\n","## <div style=\"box-shadow: rgba(0, 0, 0, 0.18) 0px 2px 4px inset; padding:20px; font-size:24px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(67, 66, 66)\"> <b> 10.1 Encode prompts</b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embeddings2 = st_model.encode(prompts).flatten()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<a id=\"10\"></a>\n","# <div style=\"box-shadow: rgba(0, 0, 0, 0.16) 0px 1px 4px inset, rgb(51, 51, 51) 0px 0px 0px 3px inset; padding:20px; font-size:32px; font-family: consolas; text-align:center; display:fill; border-radius:15px;  color:rgb(34, 34, 34);\"> <b> 11. Release GPU resource </b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del ci\n","del blip_model, clip_model\n","del st_model\n","torch.cuda.empty_cache()\n","gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### <a id=\"top\"></a>\n","# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b> Ensemble(1) </b></div>\n"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embeddings12 = ratio_OFA * embeddings1 + ratio_CLIP_Interrogator * embeddings2"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["del embeddings1\n","del embeddings2\n","gc.collect()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### <a id=\"top\"></a>\n","# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b> ViT-B-16 </b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","from pathlib import Path\n","from PIL import Image\n","from tqdm.notebook import tqdm\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import timm\n","from sklearn.preprocessing import normalize"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class CFG:\n","    model_path = '/kaggle/input/stable-diffusion-vit-baseline-train/vit_base_patch16_224.pth'\n","    model_name = 'vit_base_patch16_224'\n","    input_size = 224\n","    batch_size = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class DiffusionTestDataset(Dataset):\n","    def __init__(self, images, transform):\n","        self.images = images\n","        self.transform = transform\n","    \n","    def __len__(self):\n","        return len(self.images)\n","\n","    def __getitem__(self, idx):\n","        image = Image.open(self.images[idx])\n","        image = self.transform(image)\n","        return image"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def predict(\n","    images,\n","    model_path,\n","    model_name,\n","    input_size,\n","    batch_size\n","):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    transform = transforms.Compose([\n","        transforms.Resize(input_size),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","#         transforms.RandomRotation(degrees=10),\n","\n","        # transforms.RandomVerticalFlip(p=0.5),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n","    ])\n","    dataset = DiffusionTestDataset(images, transform)\n","    dataloader = DataLoader(\n","        dataset=dataset,\n","        shuffle=False,\n","        batch_size=batch_size,\n","        pin_memory=True,\n","        num_workers=2,\n","        drop_last=False\n","    )\n","\n","    model = timm.create_model(\n","        model_name,\n","        pretrained=False,\n","        num_classes=384\n","    )\n","    state_dict = torch.load(model_path)\n","    model.load_state_dict(state_dict)\n","    model.to(device)\n","    model.eval()\n","    \n","    tta_preds = None\n","    for _ in range(2):\n","        preds = []\n","        for X in tqdm(dataloader, leave=False):\n","            X = X.to(device)\n","\n","            with torch.no_grad():\n","                X_out = model(X).cpu().numpy()\n","                # L2 normalize -- Start\n","                X_out = X_out / ( np.abs(X_out).max(axis=-1, keepdims=True) + 0.0000001)  # To avoid to overflow at normalize()\n","                X_out = normalize( X_out )\n","                # L2 normalize -- End\n","                preds.append(X_out)\n","                \n","        if tta_preds is None:\n","            tta_preds = np.vstack(preds).flatten()\n","        else:\n","            tta_preds += np.vstack(preds).flatten()\n","    \n","    return tta_preds / 2"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["images = list(Path('/kaggle/input/stable-diffusion-image-to-prompts/images').glob('*.png'))\n","imgIds = [i.stem for i in images]\n","EMBEDDING_LENGTH = 384\n","imgId_eId = [\n","    '_'.join(map(str, i)) for i in zip(\n","        np.repeat(imgIds, EMBEDDING_LENGTH),\n","        np.tile(range(EMBEDDING_LENGTH), len(imgIds)))]\n","\n","embeddings3 = predict(images, CFG.model_path, CFG.model_name, CFG.input_size, CFG.batch_size)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### <a id=\"top\"></a>\n","# <div style=\"box-shadow: rgb(60, 121, 245) 0px 0px 0px 3px inset, rgb(255, 255, 255) 10px -10px 0px -3px, rgb(31, 193, 27) 10px -10px, rgb(255, 255, 255) 20px -20px 0px -3px, rgb(255, 217, 19) 20px -20px, rgb(255, 255, 255) 30px -30px 0px -3px, rgb(255, 156, 85) 30px -30px, rgb(255, 255, 255) 40px -40px 0px -3px, rgb(255, 85, 85) 40px -40px; padding:20px; margin-right: 40px; font-size:30px; font-family: consolas; text-align:center; display:fill; border-radius:15px; color:rgb(60, 121, 245);\"><b> Ensemble(2) </b></div>"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["embeddings = embeddings12 + ratio_ViT_B_16 * embeddings3"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission = pd.DataFrame(\n","    index=imgId_eId,\n","    data=embeddings,\n","    columns=['val']\n",").rename_axis('imgId_eId')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["submission.to_csv('submission.csv')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"}},"nbformat":4,"nbformat_minor":4}
